Authors introduce the **Outdoor Hazard Detection** dataset, a valuable resource for the advancement of outdoor risk detection systems tailored for low-cost electric scooters. This dataset leverages the **“You Only Look at Interested Cells” (YOLIC)** framework to facilitate efficient real-time hazard detection during scooter navigation. With a total of 20,380 images, the dataset is divided into *test* (4,076 images), *train* (14,266 images), and *val* (2,038 images) sets, encompassing 11 distinct hazard classes, including but not limited to *bumps*, *weeds*, *columns*, and *fences*. It serves as a crucial tool for the development and evaluation of safety measures in the context of electric scooter travel.

## About YOLIC

YOLIC employs Cell of Interest (CoI) as the unit of classification based on cell-wise segmentation rather than classifying each pixel as in semantic segmentation. These CoIs, larger than individual pixels, encapsulate more relevant information, thereby enhancing detection efficiency and reducing computational overhead. On the other hand, instead of merely enclosing objects within bounding boxes, YOLIC infers a rough shape of the objects by utilizing multiple CoIs, offering a more nuanced perspective of object presence in the scene. This is accomplished by defining a number of cells and concentrating solely on those identified as CoIs in an image. Thus computational load could be significantly reduced. YOLIC is particularly effective in scenarios where both the shape and location of an object are crucial. Leveraging a prior knowledge of the expected object’s position, size, and shape, YOLIC allows for predefined positioning and count of cells. This approach simplifies the computational process for object detection, eliminating the need for region proposal networks or bounding box regression techniques. Furthermore, YOLIC adopts multi-label classification for each cell, enhancing the detection of multiple objects within a single cell. This approach effectively recognizes overlapping or closely situated objects within the same cell, thereby overcoming the limitations associated with single-label classification. YOLIC’s integration of cell-wise segmentation and multi-label classification achieves accuracy levels comparable to state-of-theart object detection algorithms while significantly reducing computational resource demands. This balance between precision and computational efficiency positions YOLIC as an ideal solution for object detection on edge devices.

<img src="https://i.ibb.co/pKjYV55/Screenshot-2023-10-19-132839.png" alt="image" width="800">

<span style="font-size: smaller; font-style: italic;">YOLIC’s adaptability is showcased in diverse applications such as intelligent driving, industrial manufacturing, and smart parking, where it proficiently identifies various cells of interest. Unlike conventional object detection algorithms which involve laborious searching for objects in the entire image, the proposed method passively waits for the object to appear in the predefined cells of interest. This flexibility enables customized cell configurations for unique scenarios, allowing precise object detection and analysis across different tasks.</span>

“You Only Look at Interested Cells” (YOLIC) is specifically designed for edge devices and low-performance hardware. Rather than merely classifying predefined pixels of a compressed image, YOLIC utilizes a more complex process, identifying object categories by classifying CoIs of various shapes and sizes, based on the application’s requirements. This approach effectively converts each input image into a ‘mosaic art image,’ composed of numerous diverse CoIs. The cell configuration, defining the size, shape, position, and quantity of CoIs, is what lends YOLIC its precision and versatility in object localization, differing from traditional algorithms that rely primarily on object coordinates. These cells, tailored according to factors such as distance, direction, and the degree of danger posed by frequently appearing objects, provide sufficient and critical information for tasks like safe driving. For instance, in intelligent driving applications, CoIs can be set according to driving distance for accurate detection of the distance between objects and the vehicle. In industrial manufacturing scenarios, CoIs can be configured based on equipment panels for monitoring the operational status of traditional industrial machinery. Additionally, in smart parking lot applications, CoIs can be tailored to individual parking spaces to precisely locate vehicles. Furthermore, YOLIC’s adaptability extends to various other applications such as retail analytics, where CoIs can be set to monitor product shelf areas for inventory management and customer behavior analysis. In healthcare, CoIs can be adjusted to track and analyze specific body parts or regions in medical imaging for early diagnosis and treatment planning. Overall, YOLIC’s flexibility in adjusting CoIs across a multitude of tasks showcases its potential for widespread adoption in diverse industries and use cases.

<img src="https://i.ibb.co/bmDL1xD/Screenshot-2023-10-19-133442.png" alt="image" width="800">

<span style="font-size: smaller; font-style: italic;">A detailed diagram of the YOLIC model, showcasing its two main components: the feature extraction module and the multi-label classification head. The figure also demonstrates the output of the model, highlighting the position of each CoI on an input image. The overall structure emphasizes YOLIC’s lightweight and efficient design, which makes it well-suited for edge devices with limited computational resources.</span>

In this experiment, authors focus on the development of an outdoor risk detection system specifically tailored for low-cost electric scooters. The motivation behind using YOLIC for this application lies in its ability to provide efficient real-time detection, which is crucial for ensuring safety during scooter navigation. Furthermore, low-cost electric scooters typically have limited computational resources, making YOLIC’s lightweight architecture an ideal choice for deployment on such devices. To accommodate the typical speed of electric scooters and ensure safety, authors designed a detection range of 0-6 meters on the road. Authors divided this range into different CoIs based on distance, allowing for precise object localization and prioritization of areas posing higher risks. In total, 96 CoIs are allocated to the road detection area. Additionally, authors designated a traffic sign detection area at the top of the image, employing eight CoIs to approximate the location of traffic signs. For this application, it is essential to accurately identify various objects that could potentially pose risks to scooter navigation. Accordingly, authors defined 11 objects of interest: Bump, Column, Dent, Fence, People, Vehicle, Wall, Weed, Zebra Crossing, Traffic Cone, and Traffic Sign. These objects were selected due to their relevance and potential impact on scooter movement and safety. Therefore, considering the 11 objects and one additional category for background class, YOLIC’s output for this experiment becomes 104 × (11 + 1) = 1248. In authors' evaluation, authors hand-picked and annotated 20,380 distinct video frames from road footage around the University of Aizu campus. These frames were carefully chosen to provide unique data points. For the experiment, authors randomly divided this dataset into 70% for training, 10% for validation, and the remaining 20% for testing. This dataset showcasing varied outdoor conditions, facilitates a comprehensive assessment of the effectiveness of authors' proposed YOLIC-based risk detection system for electric scooters. Authors evaluate authors' model using common metrics such as precision, recall, and the F1-score for each object category. Authors also appraise the binary classification of each CoI based on the results of the background class.
